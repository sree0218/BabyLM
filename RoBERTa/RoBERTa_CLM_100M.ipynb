{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j4i0rez_IsQ3"},"outputs":[],"source":["!pip install git+https://github.com/huggingface/transformers.git\n","!pip install datasets\n","!pip install transformers torch\n","!pip install accelerate\n","!apt install git-lfs"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QMSI7CqN41A-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"jgF-gr7q4H-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","torch.cuda.empty_cache()\n","\n","from typing import Dict, Tuple\n","from datasets import list_datasets, load_dataset, DatasetDict,Dataset\n","from collections import Counter\n","from typing import List, Dict, Union, Callable, Any\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","from pprint import pprint\n","import torch\n","import torch.nn as nn\n"],"metadata":{"id":"bXlTvtxVIzrX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)     "],"metadata":{"id":"hD0uikxdI5Ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","ds_train = load_dataset(\"Sree1994/Babylm_100M\", split=\"train\")\n","ds_valid = load_dataset(\"Sree1994/Babylm_100M\", split=\"valid\")\n","\n","raw_datasets = DatasetDict(\n","    {\n","        \"train\": ds_train,\n","        \"valid\": ds_valid\n","    }\n",")\n","\n","raw_datasets"],"metadata":{"id":"8mbrNIFn0n2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","\n","context_length = 128\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","vocab_size = tokenizer.vocab_size\n","\n","outputs = tokenizer(\n","    raw_datasets[\"train\"][\"text\"],\n","    truncation=True,\n","    max_length=context_length,\n","    return_overflowing_tokens=True,\n","    return_length=True,\n","    pad_to_max_length=True,\n",")\n","\n","print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n","print(f\"Input chunk lengths: {(outputs['length'])}\")\n","print(f\"Chunk mapping: {outputs['attention_mask']}\")"],"metadata":{"id":"Vt5eKFKm2HmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(element):\n","    outputs = tokenizer(\n","        element[\"text\"],\n","        truncation=True,\n","        max_length=context_length,\n","        return_overflowing_tokens=True,\n","        return_length=True,\n","    )\n","    input_batch = []\n","    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n","        if length <= context_length:\n","            input_batch.append(input_ids)\n","    return {\"input_ids\": input_batch}\n","\n","tokenized_datasets = raw_datasets.map(\n","    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",")\n","tokenized_datasets"],"metadata":{"id":"X465uYJB3xlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n","import torch\n","\n","config = AutoConfig.from_pretrained(\n","    \"roberta-base\",\n","    vocab_size=len(tokenizer),\n","    is_decoder=True,\n","    random_init=True\n",")\n","print(len(tokenizer))\n","# model = RobertaForCausalLM.from_pretrained(\"roberta-base\", is_decoder=True, vocab_size=10_000)\n","model = RobertaForCausalLM(config).to(device)\n","# model.init_weights()\n","model_size = sum(t.numel() for t in model.parameters())\n","print(f\"RoBERTa size: {model_size/1000**2:.1f}M parameters\")\n","# print(config)"],"metadata":{"id":"CTJvdRZY48CA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")"],"metadata":{"id":"TUeh5lj56Y1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"id":"JrVa7fTRY9Vl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluate\n","from evaluate import TextClassificationEvaluator, Metric, EvaluationModuleInfo\n","class Cal_Perplexity(Metric):\n","    \"\"\"\n","    You can define custom metrics! In this case I do this to compute Macro-F1, which averages per-class F1 scores\n","    \"\"\"\n","    pp_metric_info: EvaluationModuleInfo = evaluate.load(\"perplexity\")._info()\n","\n","    def _info(self) -> EvaluationModuleInfo:\n","        # we'll just say the info is the same in this case\n","        return MyMacroF1Metric.pp_metric_info\n","\n","    def _compute(self, loss) -> Dict[str, Any]:\n","        # we can just call the sklearn implementation! Metrics in huggingface generally correspond with sklearn metrics\n","        # when applicable\n","        pp = torch.exp()\n","        return {\"perplexity\": float(pp) if pp.size == 1 else pp}"],"metadata":{"id":"T6pJupVaXvUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","# # PP: Cal_Perplexity = Cal_Perplexity()\n","# my_evaluation: Cal_Perplexity = Cal_Perplexity()\n","\n","# def Cal_Perplexity(eval_pred: EvalPrediction) -> Dict[str, float]:\n","#         logits, labels = eval_pred.predictions, eval_pred.label_ids\n","#         predictions: Tensor = logits.argmax(axis=-1)\n","#         return my_evaluation.compute(predictions=predictions, references=labels)\n","\n","args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Baby_Lm\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy = 'epoch',    \n","    do_train=True,\n","    do_eval=True,\n","    do_predict=True,\n","    per_device_train_batch_size=64,\n","    per_device_eval_batch_size=64,\n","    # evaluation_strategy=\"steps\",\n","    eval_steps=5_000,\n","    logging_steps=5_000,\n","    gradient_accumulation_steps=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    warmup_steps=1_000,\n","    lr_scheduler_type=\"cosine\",\n","    learning_rate=5e-4,\n","    save_steps=1000,\n","    fp16=True,\n","    push_to_hub=False,\n","    save_total_limit=1,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    # compute_metrics=my_compute_metrics,\n",")"],"metadata":{"id":"w46C6yUw7GNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"GXVEX9eUrX-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trn = trainer.train()\n","# model = trainer.model  # make sure to load_best_model_at_end=True!\n","\n","# run a final evaluation on the test set\n","val = trainer.evaluate(metric_key_prefix=\"test\", eval_dataset=tokenized_datasets[\"valid\"])\n","valid_loss = val.get(\"test_loss\")\n","# print(f\"Training Loss: {trn.training_loss}\")\n","print(f\"Validation Loss: {valid_loss}\")\n","print(f\"Validation Perplexity: {torch.exp(torch.tensor(valid_loss))}\")"],"metadata":{"id":"8MGN5eV_rbE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A2FoI_c2rcjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YrsrvE6xrcgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trn = trainer.train()\n","model = trainer.model  # make sure to load_best_model_at_end=True!\n","\n","# run a final evaluation on the test set\n","val = trainer.evaluate(metric_key_prefix=\"test\", eval_dataset=tokenized_datasets[\"valid\"])"],"metadata":{"id":"8Ul83FJ97st2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trn.training_loss"],"metadata":{"id":"-Pbr-TfzduxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val"],"metadata":{"id":"foZhXS1mvOWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid = val.get(\"test_loss\")\n","torch.exp(torch.tensor(valid))"],"metadata":{"id":"AZTnUMVedx67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import pipeline\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","pipe = pipeline(\"text-generation\", model=model, device=device, tokenizer=tokenizer)"],"metadata":{"id":"q5pckvYSZ-UG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe.predict(\"</s>\")"],"metadata":{"id":"84nDslnA_lxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"who is Brother Lustig?\"\n","print(pipe(text, num_return_sequences=1)[0][\"generated_text\"])\n","\n","# Input text for prediction\n","# input_text = \"who is Brother Lustig?\"\n","\n","# # Encode input text\n","# input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors='pt')\n","\n","# # Generate next token predictions\n","# next_token_logits = model(input_ids).logits[:, -1, :]\n","# next_token_id = next_token_logits.argmax().item()\n","# next_token = tokenizer.decode([next_token_id])\n","\n","# # Print next predicted token\n","# print(f\"Next token prediction: {next_token}\")"],"metadata":{"id":"qnEBjVvudxkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4OQy-8bzt811"},"execution_count":null,"outputs":[]}]}