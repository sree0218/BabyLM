{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4i0rez_IsQ3"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install transformers torch\n",
        "!pip install accelerate\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QMSI7CqN41A-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533afc07-c233-4c18-dd10-f9343dd07732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "jgF-gr7q4H-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from datasets import list_datasets, load_dataset, DatasetDict,Dataset\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Union, Callable, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "bXlTvtxVIzrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)     "
      ],
      "metadata": {
        "id": "hD0uikxdI5Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dcf49d-761d-4a18-901d-c13c95088f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset: DatasetDict = load_dataset(\"Sree1994/babylm_childstories\")\n",
        "\n",
        "ds_train = load_dataset(\"Sree1994/blm_strict_small\", split=\"train\")\n",
        "ds_valid = load_dataset(\"Sree1994/blm_strict_small\", split=\"valid\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,\n",
        "        \"valid\": ds_valid\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "8mbrNIFn0n2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c558414-d377-4955-8850-958030f7f0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/Sree1994___parquet/Sree1994--blm_strict_small-3934cc01075622aa/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/Sree1994___parquet/Sree1994--blm_strict_small-3934cc01075622aa/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        "    pad_to_max_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['attention_mask']}\")"
      ],
      "metadata": {
        "id": "Vt5eKFKm2HmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b876b7-d197-422c-ba7f-6cae3d23716b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length <= context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "X465uYJB3xlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5158849c-3891-4c00-84f0-0fd8d22be7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/Sree1994___parquet/Sree1994--blm_strict_small-3934cc01075622aa/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9066acf84e2adbf5.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/Sree1994___parquet/Sree1994--blm_strict_small-3934cc01075622aa/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bc2ea9615a48692a.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, RobertaForMaskedLM, AutoConfig\n",
        "import torch\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    is_decoder=False,\n",
        "    random_init=True,\n",
        "    no_deprecation_warning=True,\n",
        ")\n",
        "print(len(tokenizer))\n",
        "# model = RobertaForCausalLM.from_pretrained(\"roberta-base\", is_decoder=True, vocab_size=10_000)\n",
        "model = RobertaForMaskedLM(config).to(device)\n",
        "# model.init_weights()\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"RoBERTa size: {model_size/1000**2:.1f}M parameters\")\n",
        "# print(config)"
      ],
      "metadata": {
        "id": "CTJvdRZY48CA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c073b2-eda1-4397-9820-91498531679d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50265\n",
            "RoBERTa size: 124.7M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15)\n",
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ],
      "metadata": {
        "id": "TUeh5lj56Y1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5e2dbe-9ac2-46da-8354-264d452969ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([5, 128])\n",
            "attention_mask shape: torch.Size([5, 128])\n",
            "labels shape: torch.Size([5, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Baby_Lm/BLM_Roberta_Baseline_MLM\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',    \n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=True,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=1000,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        "    # compute_metrics=my_compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "w46C6yUw7GNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "Nhh1WsfwJOtv",
        "outputId": "4fd05a85-cce7-445d-bc3d-624e1999afe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 52:03, Epoch 14/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.618752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.481133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.558644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.223675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.085870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.015002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.939378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.795554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.772528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.684867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.615674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.570890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.509982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.342680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.194887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=7.2539855435363245, metrics={'train_runtime': 3135.602, 'train_samples_per_second': 95.675, 'train_steps_per_second': 0.187, 'total_flos': 1.974273780712781e+16, 'train_loss': 7.2539855435363245, 'epoch': 15.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trn = trainer.train()\n",
        "# model = trainer.model  # make sure to load_best_model_at_end=True!\n",
        "\n",
        "# run a final evaluation on the test set\n",
        "val = trainer.evaluate(metric_key_prefix=\"test\", eval_dataset=tokenized_datasets[\"valid\"])\n",
        "valid_loss = val.get(\"test_loss\")\n",
        "# print(f\"Training Loss: {trn.training_loss}\")\n",
        "print(f\"Validation Loss: {valid_loss}\")\n",
        "print(f\"Validation Perplexity: {torch.exp(torch.tensor(valid_loss))}\")"
      ],
      "metadata": {
        "id": "8Ul83FJ97st2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "98e718e5-6602-418c-9fbb-4d76e18ebab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [79/79 00:17]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 6.185730457305908\n",
            "Validation Perplexity: 485.7676696777344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Validation Perplexity: {torch.exp(torch.tensor(5.40))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg-CM8GLn_1y",
        "outputId": "68659ec0-e4b4-408a-ab72-630ee467350b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Validation Perplexity: 221.40643310546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trn.metrics"
      ],
      "metadata": {
        "id": "ijGYO3ow81oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "wnEfvOYNMasC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's try some Predictions "
      ],
      "metadata": {
        "id": "a30T0VIKx73u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import pipeline\n",
        "\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# pipe = pipeline(\n",
        "#     \"text-generation\", model=\"Sree1994/BLM_Roberta_Baseline\", device=device\n",
        "# )"
      ],
      "metadata": {
        "id": "PTh3Ffqh-Eqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import pipeline\n",
        "\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# pipe = pipeline(\"text-generation\", model=model, device=device, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "q5pckvYSZ-UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipe.predict(\"</s>\")"
      ],
      "metadata": {
        "id": "84nDslnA_lxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"who is Brother Lustig?\"\n",
        "# print(pipe(text, num_return_sequences=1)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "qnEBjVvudxkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's collect some graphs"
      ],
      "metadata": {
        "id": "ldodELc6yATG"
      }
    }
  ]
}