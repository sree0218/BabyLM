{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1uItzZ2oOft6LqwSwhFzR7yFB0IPRlgyb","authorship_tag":"ABX9TyMjvc2YJnCr9qCXddpm7PkZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-5u-4cmqlE5J"},"outputs":[],"source":["!pip install git+https://github.com/huggingface/transformers.git\n","!pip install datasets\n","!pip install transformers torch\n","!pip install sentencepiece"]},{"cell_type":"code","source":["from datasets import list_datasets, load_dataset, DatasetDict, Dataset\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"ZQjRjG1FlbrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip install huggingface_hub\n","!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""],"metadata":{"id":"RvhQ-eW5znCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_names = ['aochildes','bnc_spoken','cbt', 'children_stories', 'gutenberg',\n","              'open_subtitles', 'qed', 'simple_wikipedia', 'simple_wikipedia', 'wikipedia']"],"metadata":{"id":"S1C-3iFDwQOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#10M data\n","all_datasets=[]\n","for filename in file_names:  # replace with way of listing files names\n","  with open(f'/content/drive/MyDrive/NLP244/244_Project_Folder/data/babylm_data/babylm_10M/{filename}.train', 'r') as f:  # adjust as needed\n","      lines_tr = f.readlines()\n","\n","train, valid = train_test_split(lines_tr, test_size=0.2, random_state=42)    \n","print(len(train),len(valid))\n","dataset1 = Dataset.from_list([{'text': t} for t in train])\n","dataset2 = Dataset.from_list([{'text': t} for t in valid])\n","temp=DatasetDict({\"train\": dataset1, \"valid\": dataset2})\n","temp.push_to_hub(f\"Sree1994/blm_strict_small\")"],"metadata":{"id":"Wv2WaIeHf6tc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["snli_ds = load_dataset('Sree1994/blm_strict_small')  #loading snli dataset\n","print(snli_ds)"],"metadata":{"id":"3ZTGOWRZlgu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#100M data\n","all_datasets=[]\n","for filename in file_names:  # replace with way of listing files names\n","  with open(f'/content/drive/MyDrive/NLP244/244_Project_Folder/data/babylm_data/babylm_100M/{filename}.train', 'r') as f:  # adjust as needed\n","      lines_tr = f.readlines()\n","      # all_datasets.extend(lines)\n","\n","for filename in file_names:  # replace with way of listing files names\n","  with open(f'/content/drive/MyDrive/NLP244/244_Project_Folder/data/babylm_data/babylm_dev/{filename}.dev', 'r') as f:  # adjust as needed\n","      lines_dev = f.readlines()\n","\n","for filename in file_names:  # replace with way of listing files names\n","  with open(f'/content/drive/MyDrive/NLP244/244_Project_Folder/data/babylm_data/babylm_test/{filename}.test', 'r') as f:  # adjust as needed\n","      lines_test = f.readlines()\n","\n","\n","dataset1 = Dataset.from_list([{'text': t} for t in lines_tr])\n","dataset2 = Dataset.from_list([{'text': t} for t in lines_test])\n","dataset3 = Dataset.from_list([{'text': t} for t in lines_dev])\n","temp=DatasetDict({\"train\": dataset1, \"test\":dataset2, \"valid\":dataset3})\n","temp.push_to_hub(f\"Sree1994/Babylm_100M\")\n"],"metadata":{"id":"k8Kc8L98wLIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["snli_ds = load_dataset('Sree1994/Babylm_100M')  #loading snli dataset\n","print(snli_ds)"],"metadata":{"id":"AgiS_1fQmzK3"},"execution_count":null,"outputs":[]}]}